{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [beta0, beta1, beta2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15645573154479397223f17bc11082d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Not enough samples to build a trace.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m     y_obs \u001b[38;5;241m=\u001b[39m pm\u001b[38;5;241m.\u001b[39mBernoulli(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_obs\u001b[39m\u001b[38;5;124m'\u001b[39m, p\u001b[38;5;241m=\u001b[39mp, observed\u001b[38;5;241m=\u001b[39mY)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# 4. Sampling - creștem target_accept pentru a reduce divergențele\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     trace \u001b[38;5;241m=\u001b[39m \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_accept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Rezumat a posteriori\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(az\u001b[38;5;241m.\u001b[39msummary(trace, var_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta2\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\ddanc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pymc\\sampling\\mcmc.py:875\u001b[0m, in \u001b[0;36msample\u001b[1;34m(draws, tune, chains, cores, random_seed, progressbar, progressbar_theme, step, var_names, nuts_sampler, initvals, init, jitter_max_retries, n_init, trace, discard_tuned_samples, compute_convergence_checks, keep_warning_stat, return_inferencedata, idata_kwargs, nuts_sampler_kwargs, callback, mp_ctx, blas_cores, model, **kwargs)\u001b[0m\n\u001b[0;32m    871\u001b[0m t_sampling \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t_start\n\u001b[0;32m    873\u001b[0m \u001b[38;5;66;03m# Packaging, validating and returning the result was extracted\u001b[39;00m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;66;03m# into a function to make it easier to test and refactor.\u001b[39;00m\n\u001b[1;32m--> 875\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_return\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt_sampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_sampling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_warning_stat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_warning_stat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43midata_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midata_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ddanc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pymc\\sampling\\mcmc.py:906\u001b[0m, in \u001b[0;36m_sample_return\u001b[1;34m(run, traces, tune, t_sampling, discard_tuned_samples, compute_convergence_checks, return_inferencedata, keep_warning_stat, idata_kwargs, model)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;66;03m# Pick and slice chains to keep the maximum number of samples\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m discard_tuned_samples:\n\u001b[1;32m--> 906\u001b[0m     traces, length \u001b[38;5;241m=\u001b[39m \u001b[43m_choose_chains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    908\u001b[0m     traces, length \u001b[38;5;241m=\u001b[39m _choose_chains(traces, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ddanc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pymc\\backends\\base.py:593\u001b[0m, in \u001b[0;36m_choose_chains\u001b[1;34m(traces, tune)\u001b[0m\n\u001b[0;32m    591\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(trace) \u001b[38;5;241m-\u001b[39m tune) \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m traces]\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(lengths):\n\u001b[1;32m--> 593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough samples to build a trace.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    595\u001b[0m idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(lengths)\n\u001b[0;32m    596\u001b[0m l_sort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(lengths)[idxs]\n",
      "\u001b[1;31mValueError\u001b[0m: Not enough samples to build a trace."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "# 1. Încărcăm datele\n",
    "data = pd.read_csv(\"Admission.csv\")\n",
    "\n",
    "Y = data['Admission'].values\n",
    "GRE = data['GRE'].values\n",
    "GPA = data['GPA'].values\n",
    "\n",
    "# 2. Normalizare (standardizare) a predictorilor\n",
    "GRE_mean, GRE_std = GRE.mean(), GRE.std()\n",
    "GPA_mean, GPA_std = GPA.mean(), GPA.std()\n",
    "\n",
    "GRE_std_data = (GRE - GRE_mean) / GRE_std\n",
    "GPA_std_data = (GPA - GPA_mean) / GPA_std\n",
    "\n",
    "# 3. Construim modelul Bayesian cu priors slab informative\n",
    "with pm.Model() as model:\n",
    "    # Priori slab informative pentru parametri\n",
    "    beta0 = pm.Normal('beta0', mu=0, sigma=10)\n",
    "    beta1 = pm.Normal('beta1', mu=0, sigma=10)\n",
    "    beta2 = pm.Normal('beta2', mu=0, sigma=10)\n",
    "\n",
    "    # Logit-ul probabilității\n",
    "    logits = beta0 + beta1 * GRE_std_data + beta2 * GPA_std_data\n",
    "    p = pm.math.sigmoid(logits)\n",
    "\n",
    "    # Likelihood\n",
    "    y_obs = pm.Bernoulli('y_obs', p=p, observed=Y)\n",
    "\n",
    "    # 4. Sampling - creștem target_accept pentru a reduce divergențele\n",
    "    trace = pm.sample(3000, tune=2000, target_accept=0.95, return_inferencedata=True)\n",
    "\n",
    "# Rezumat a posteriori\n",
    "print(az.summary(trace, var_names=[\"beta0\", \"beta1\", \"beta2\"]))\n",
    "\n",
    "# 2B\n",
    "\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Presupunem că aveți deja datele și modelul rulat:\n",
    "# Y: Admission (0/1)\n",
    "# GRE, GPA: datele originale\n",
    "# trace: rezultatul PyMC (inferencedata) cu parametrii beta0, beta1, beta2\n",
    "\n",
    "# Presupunem că ați salvat în prealabil mediile și deviatia standard pentru normalizare:\n",
    "# GRE_mean, GRE_std, GPA_mean, GPA_std\n",
    "\n",
    "# Eșantioane a posteriori\n",
    "beta0_s = trace.posterior['beta0'].values.flatten()\n",
    "beta1_s = trace.posterior['beta1'].values.flatten()\n",
    "beta2_s = trace.posterior['beta2'].values.flatten()\n",
    "\n",
    "# Definim un interval pe axa GRE în scara originală\n",
    "gre_range = np.linspace(GRE.min(), GRE.max(), 100)\n",
    "\n",
    "# Convertim aceste valori în scara standardizată, pentru a aplica formula frontierei\n",
    "gre_range_std = (gre_range - GRE_mean) / GRE_std\n",
    "\n",
    "# Pentru fiecare valoare GRE, calculăm GPA_std la p=0.5:\n",
    "# 0 = beta0 + beta1*GRE_std + beta2*GPA_std\n",
    "# => GPA_std = -(beta0 + beta1*GRE_std)/beta2\n",
    "all_boundaries_std = []\n",
    "for gre_val_std in gre_range_std:\n",
    "    gpa_vals_std = -(beta0_s + beta1_s * gre_val_std) / beta2_s\n",
    "    all_boundaries_std.append(gpa_vals_std)\n",
    "\n",
    "all_boundaries_std = np.vstack(all_boundaries_std)  # (100, nr_samples)\n",
    "\n",
    "# Mediană și HDI pe scala standardizată\n",
    "median_line_std = np.median(all_boundaries_std, axis=1)\n",
    "\n",
    "hdi_lower_std = []\n",
    "hdi_upper_std = []\n",
    "for row in all_boundaries_std:\n",
    "    interval = az.hdi(row, hdi_prob=0.94)\n",
    "    hdi_lower_std.append(interval[0])\n",
    "    hdi_upper_std.append(interval[1])\n",
    "\n",
    "hdi_lower_std = np.array(hdi_lower_std)\n",
    "hdi_upper_std = np.array(hdi_upper_std)\n",
    "\n",
    "# Denormalizare înapoi la scara originală a GPA:\n",
    "median_line = median_line_std * GPA_std + GPA_mean\n",
    "hdi_lower = hdi_lower_std * GPA_std + GPA_mean\n",
    "hdi_upper = hdi_upper_std * GPA_std + GPA_mean\n",
    "\n",
    "# Acum putem afișa graficul pe scara originală\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(gre_range, median_line, color='red', label='Median decision boundary')\n",
    "plt.fill_between(gre_range, hdi_lower, hdi_upper, color='red', alpha=0.3, label='94% HDI')\n",
    "\n",
    "# Datele originale\n",
    "plt.scatter(GRE, GPA, c=Y, cmap='bwr', alpha=0.6, label='Data (color: admission)')\n",
    "\n",
    "plt.xlabel(\"GRE\")\n",
    "plt.ylabel(\"GPA\")\n",
    "plt.title(\"Decision Boundary with 94% HDI (original scale)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#c\n",
    "# Datele pentru studentul cu GRE=550 și GPA=3.5\n",
    "GRE_value_1 = 550\n",
    "GPA_value_1 = 3.5\n",
    "\n",
    "# Transformăm aceste valori la scara standardizată\n",
    "GRE_value_std_1 = (GRE_value_1 - GRE_mean) / GRE_std\n",
    "GPA_value_std_1 = (GPA_value_1 - GPA_mean) / GPA_std\n",
    "\n",
    "# Calculăm logit-ul pentru acest student\n",
    "logit_student_1 = beta0_s + beta1_s * GRE_value_std_1 + beta2_s * GPA_value_std_1\n",
    "\n",
    "# Calculăm probabilitățile de admitere (sigmoid) pentru fiecare eșantion\n",
    "p_student_1 = 1 / (1 + np.exp(-logit_student_1))\n",
    "\n",
    "# Calculăm intervalul HDI de 90% pentru probabilitatea de admitere\n",
    "hdi_student_90_1 = az.hdi(p_student_1, hdi_prob=0.90)\n",
    "print(f\"Interval HDI de 90% pentru probabilitatea de admitere pentru un student cu GRE={GRE_value_1} și GPA={GPA_value_1}: {hdi_student_90_1}\")\n",
    "\n",
    "# Creăm o rețea de puncte pentru a calcula probabilitatea de admitere\n",
    "GRE_range = np.linspace(GRE.min(), GRE.max(), 50)  # 50 de puncte între min și max GRE\n",
    "GPA_range = np.linspace(GPA.min(), GPA.max(), 50)  # 50 de puncte între min și max GPA\n",
    "\n",
    "# Creăm o rețea de puncte pentru a calcula probabilitatea de admitere\n",
    "GRE_grid, GPA_grid = np.meshgrid(GRE_range, GPA_range)\n",
    "GRE_grid_std = (GRE_grid - GRE_mean) / GRE_std\n",
    "GPA_grid_std = (GPA_grid - GPA_mean) / GPA_std\n",
    "\n",
    "# Calculăm logit(p) pentru fiecare punct din rețea\n",
    "logit_p_grid = beta0_s[:, None, None] + beta1_s[:, None, None] * GRE_grid_std + beta2_s[:, None, None] * GPA_grid_std\n",
    "\n",
    "# Aplicăm sigmoid pentru a obține probabilitățile\n",
    "p_grid = 1 / (1 + np.exp(-logit_p_grid))\n",
    "\n",
    "# Calculăm intervalul HDI pentru fiecare punct din rețea\n",
    "hdi_lower_grid = np.percentile(p_grid, 5, axis=0)\n",
    "hdi_upper_grid = np.percentile(p_grid, 95, axis=0)\n",
    "\n",
    "# Calculăm probabilitatea medie pentru fiecare combinație de GRE și GPA\n",
    "median_p_grid = np.median(p_grid, axis=0)\n",
    "\n",
    "# Vizualizarea graficului 2D\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot pentru datele originale, colorate după probabilitatea de admitere\n",
    "scatter = plt.scatter(GRE, GPA, c=Y, cmap='bwr', alpha=0.6, label='Data (color: admission)')\n",
    "\n",
    "# Contur pentru intervalul HDI al probabilității\n",
    "plt.contour(GRE_grid, GPA_grid, hdi_lower_grid, levels=[0.5], colors='red', linestyles='--', label=\"HDI lower bound (5%)\")\n",
    "plt.contour(GRE_grid, GPA_grid, hdi_upper_grid, levels=[0.5], colors='blue', linestyles='--', label=\"HDI upper bound (95%)\")\n",
    "\n",
    "# Colormap pentru probabilitatea de admitere\n",
    "plt.imshow(median_p_grid, extent=[GRE.min(), GRE.max(), GPA.min(), GPA.max()], origin='lower', aspect='auto', alpha=0.3, cmap='viridis')\n",
    "plt.colorbar(label='Median Probability of Admission')\n",
    "\n",
    "# Marcarea intervalului HDI de 90% pentru studentul cu GRE=550 și GPA=3.5\n",
    "plt.axvline(x=GRE_value_1, color='orange', linestyle='--', label=f'GRE={GRE_value_1}')\n",
    "plt.axhline(y=GPA_value_1, color='orange', linestyle='--', label=f'GPA={GPA_value_1}')\n",
    "\n",
    "# Etichete și titlu\n",
    "plt.xlabel('GRE Score')\n",
    "plt.ylabel('GPA')\n",
    "plt.title('GRE, GPA și Probabilitatea de Admitere cu HDI (90%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#d\n",
    "# Datele pentru studentul cu GRE=500 și GPA=3.2\n",
    "GRE_value_2 = 500\n",
    "GPA_value_2 = 3.2\n",
    "\n",
    "# Transformăm aceste valori la scara standardizată\n",
    "GRE_value_std_2 = (GRE_value_2 - GRE_mean) / GRE_std\n",
    "GPA_value_std_2 = (GPA_value_2 - GPA_mean) / GPA_std\n",
    "\n",
    "# Calculăm logit-ul pentru acest student\n",
    "logit_student_2 = beta0_s + beta1_s * GRE_value_std_2 + beta2_s * GPA_value_std_2\n",
    "\n",
    "# Calculăm probabilitățile de admitere (sigmoid) pentru fiecare eșantion\n",
    "p_student_2 = 1 / (1 + np.exp(-logit_student_2))\n",
    "\n",
    "# Calculăm intervalul HDI de 90% pentru probabilitatea de admitere\n",
    "hdi_student_90_2 = az.hdi(p_student_2, hdi_prob=0.90)\n",
    "print(f\"Interval HDI de 90% pentru probabilitatea de admitere pentru un student cu GRE={GRE_value_2} și GPA={GPA_value_2}: {hdi_student_90_2}\")\n",
    "\n",
    "# Creăm o rețea de puncte pentru a calcula probabilitatea de admitere\n",
    "GRE_range = np.linspace(GRE.min(), GRE.max(), 50)  # 50 de puncte între min și max GRE\n",
    "GPA_range = np.linspace(GPA.min(), GPA.max(), 50)  # 50 de puncte între min și max GPA\n",
    "\n",
    "# Creăm o rețea de puncte pentru a calcula probabilitatea de admitere\n",
    "GRE_grid, GPA_grid = np.meshgrid(GRE_range, GPA_range)\n",
    "GRE_grid_std = (GRE_grid - GRE_mean) / GRE_std\n",
    "GPA_grid_std = (GPA_grid - GPA_mean) / GPA_std\n",
    "\n",
    "# Calculăm logit(p) pentru fiecare punct din rețea\n",
    "logit_p_grid = beta0_s[:, None, None] + beta1_s[:, None, None] * GRE_grid_std + beta2_s[:, None, None] * GPA_grid_std\n",
    "\n",
    "# Aplicăm sigmoid pentru a obține probabilitățile\n",
    "p_grid = 1 / (1 + np.exp(-logit_p_grid))\n",
    "\n",
    "# Calculăm intervalul HDI pentru fiecare punct din rețea\n",
    "hdi_lower_grid = np.percentile(p_grid, 5, axis=0)\n",
    "hdi_upper_grid = np.percentile(p_grid, 95, axis=0)\n",
    "\n",
    "# Calculăm probabilitatea medie pentru fiecare combinație de GRE și GPA\n",
    "median_p_grid = np.median(p_grid, axis=0)\n",
    "\n",
    "# Vizualizarea graficului 2D\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot pentru datele originale, colorate după probabilitatea de admitere\n",
    "scatter = plt.scatter(GRE, GPA, c=Y, cmap='bwr', alpha=0.6, label='Data (color: admission)')\n",
    "\n",
    "# Contur pentru intervalul HDI al probabilității\n",
    "plt.contour(GRE_grid, GPA_grid, hdi_lower_grid, levels=[0.5], colors='red', linestyles='--', label=\"HDI lower bound (5%)\")\n",
    "plt.contour(GRE_grid, GPA_grid, hdi_upper_grid, levels=[0.5], colors='blue', linestyles='--', label=\"HDI upper bound (95%)\")\n",
    "\n",
    "# Colormap pentru probabilitatea de admitere\n",
    "plt.imshow(median_p_grid, extent=[GRE.min(), GRE.max(), GPA.min(), GPA.max()], origin='lower', aspect='auto', alpha=0.3, cmap='viridis')\n",
    "plt.colorbar(label='Median Probability of Admission')\n",
    "\n",
    "# Marcarea intervalului HDI de 90% pentru studentul cu GRE=500 și GPA=3.2\n",
    "plt.axvline(x=GRE_value_2, color='orange', linestyle='--', label=f'GRE={GRE_value_2}')\n",
    "plt.axhline(y=GPA_value_2, color='orange', linestyle='--', label=f'GPA={GPA_value_2}')\n",
    "\n",
    "# Etichete și titlu\n",
    "plt.xlabel('GRE Score')\n",
    "plt.ylabel('GPA')\n",
    "plt.title('GRE, GPA și Probabilitatea de Admitere cu HDI (90%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
